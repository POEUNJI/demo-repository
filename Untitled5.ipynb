{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff5274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/f1/95/cc2c6d79df8f113bdc6c99cdec985a878768120d87d839a34da4bd3ff90a/tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78bced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 988, in load\r\n",
      "    with _open_file_like(f, 'rb') as opened_file:\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 437, in _open_file_like\r\n",
      "    return _open_file(name_or_buffer, mode)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 418, in __init__\r\n",
      "    super().__init__(open(name, mode))\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'out-eunji/ckpt.pt'\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9e601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/work/po/nanoGPT-master/out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585c8af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 23, in <module>\r\n",
      "    exec(open('configurator.py').read()) # overrides from command line or config file\r\n",
      "  File \"<string>\", line 47, in <module>\r\n",
      "ValueError: Unknown config key: checkpoint\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \" --checkpoint=checkpoint.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54665ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 23, in <module>\r\n",
      "    exec(open('configurator.py').read()) # overrides from command line or config file\r\n",
      "  File \"<string>\", line 47, in <module>\r\n",
      "ValueError: Unknown config key: checkpoint\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \" --checkpoint=iter_0500.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9d07f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 988, in load\r\n",
      "    with _open_file_like(f, 'rb') as opened_file:\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 437, in _open_file_like\r\n",
      "    return _open_file(name_or_buffer, mode)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 418, in __init__\r\n",
      "    super().__init__(open(name, mode))\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'out-eunji/은지.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61147de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1030, in load\r\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1248, in _legacy_load\r\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\r\n",
      "_pickle.UnpicklingError: invalid load key, '\\xec'.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1436c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512\r\n",
      "-rw-r--r-- 1 work work 398 Apr 15 23:02 은지.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/work/po/nanoGPT-master/out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548b4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe144a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1030, in load\r\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1248, in _legacy_load\r\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\r\n",
      "_pickle.UnpicklingError: invalid load key, '\\xec'.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb197d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_eunji.py:\n",
      "out_dir = 'out-eunji'\n",
      "eval_interval = 10\n",
      "log_interval = 1\n",
      "\n",
      "always_save_checkpoint = False\n",
      "\n",
      "dataset = '은지말'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 8\n",
      "block_size = 64\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "max_iters = 10\n",
      "lr_decay_iters = 500\n",
      "dropout = 0.1\n",
      "\n",
      "learning_rate = 1e-3\n",
      "device = 'cpu'  # 은지는 지금 IPU 세션이니까 CPU로 지정해\n",
      "compile = False\n",
      "eval_iters = 20\n",
      "\n",
      "\n",
      "\n",
      "tokens per iteration will be: 512\n",
      "found vocab_size = 44 (inside data/은지말/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.79M\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "num decayed parameter tensors: 18, with 800,256 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "iter 0: loss 3.8638, time 56.51ms, mfu -100.00%\n",
      "iter 1: loss 3.8690, time 46.50ms, mfu -100.00%\n",
      "iter 2: loss 3.8488, time 42.95ms, mfu -100.00%\n",
      "iter 3: loss 3.8557, time 43.16ms, mfu -100.00%\n",
      "iter 4: loss 3.8438, time 42.86ms, mfu -100.00%\n",
      "iter 5: loss 3.8228, time 43.40ms, mfu 0.02%\n",
      "iter 6: loss 3.8428, time 43.22ms, mfu 0.02%\n",
      "iter 7: loss 3.8376, time 42.62ms, mfu 0.02%\n",
      "iter 8: loss 3.8292, time 44.25ms, mfu 0.02%\n",
      "iter 9: loss 3.8119, time 43.29ms, mfu 0.02%\n",
      "iter 10: loss 3.7833, time 43.21ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_eunji.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca618e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512\r\n",
      "-rw-r--r-- 1 work work 398 Apr 15 23:02 은지.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a428eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_eunji.py:\n",
      "out_dir = 'out-eunji'\n",
      "eval_interval = 10\n",
      "log_interval = 1\n",
      "\n",
      "always_save_checkpoint = False\n",
      "\n",
      "dataset = '은지말'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 8\n",
      "block_size = 64\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "max_iters = 10\n",
      "lr_decay_iters = 500\n",
      "dropout = 0.1\n",
      "\n",
      "learning_rate = 1e-3\n",
      "device = 'cpu'  # 은지는 지금 IPU 세션이니까 CPU로 지정해\n",
      "compile = False\n",
      "eval_iters = 20\n",
      "\n",
      "\n",
      "\n",
      "tokens per iteration will be: 512\n",
      "found vocab_size = 44 (inside data/은지말/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.79M\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "num decayed parameter tensors: 18, with 800,256 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "iter 0: loss 3.8638, time 57.51ms, mfu -100.00%\n",
      "iter 1: loss 3.8690, time 45.29ms, mfu -100.00%\n",
      "iter 2: loss 3.8488, time 43.88ms, mfu -100.00%\n",
      "iter 3: loss 3.8557, time 43.74ms, mfu -100.00%\n",
      "iter 4: loss 3.8438, time 43.17ms, mfu -100.00%\n",
      "iter 5: loss 3.8228, time 43.34ms, mfu 0.02%\n",
      "iter 6: loss 3.8428, time 42.73ms, mfu 0.02%\n",
      "iter 7: loss 3.8376, time 42.96ms, mfu 0.02%\n",
      "iter 8: loss 3.8292, time 44.22ms, mfu 0.02%\n",
      "iter 9: loss 3.8119, time 43.25ms, mfu 0.02%\n",
      "iter 10: loss 3.7833, time 42.74ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_eunji.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db85ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512\r\n",
      "-rw-r--r-- 1 work work 398 Apr 15 23:02 은지.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac23ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1030, in load\r\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1248, in _legacy_load\r\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\r\n",
      "_pickle.UnpicklingError: invalid load key, '\\xec'.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4734d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_eunji.py:\n",
      "out_dir = 'out-eunji'\n",
      "eval_interval = 10\n",
      "log_interval = 1\n",
      "\n",
      "always_save_checkpoint = False\n",
      "\n",
      "dataset = '은지말'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 8\n",
      "block_size = 64\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "max_iters = 10\n",
      "lr_decay_iters = 500\n",
      "dropout = 0.1\n",
      "\n",
      "learning_rate = 1e-3\n",
      "device = 'cpu'  # 은지는 지금 IPU 세션이니까 CPU로 지정해\n",
      "compile = False\n",
      "eval_iters = 20\n",
      "\n",
      "\n",
      "\n",
      "tokens per iteration will be: 512\n",
      "found vocab_size = 44 (inside data/은지말/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.79M\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "num decayed parameter tensors: 18, with 800,256 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "iter 0: loss 3.8638, time 55.53ms, mfu -100.00%\n",
      "iter 1: loss 3.8690, time 46.25ms, mfu -100.00%\n",
      "iter 2: loss 3.8488, time 43.20ms, mfu -100.00%\n",
      "iter 3: loss 3.8557, time 43.49ms, mfu -100.00%\n",
      "iter 4: loss 3.8438, time 43.07ms, mfu -100.00%\n",
      "iter 5: loss 3.8228, time 43.02ms, mfu 0.02%\n",
      "iter 6: loss 3.8428, time 43.05ms, mfu 0.02%\n",
      "iter 7: loss 3.8376, time 43.03ms, mfu 0.02%\n",
      "iter 8: loss 3.8292, time 44.01ms, mfu 0.02%\n",
      "iter 9: loss 3.8119, time 42.89ms, mfu 0.02%\n",
      "iter 10: loss 3.7833, time 43.02ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_eunji.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35912103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512\r\n",
      "-rw-r--r-- 1 work work 398 Apr 15 23:02 은지.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0133d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1030, in load\r\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1248, in _legacy_load\r\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\r\n",
      "_pickle.UnpicklingError: invalid load key, '\\xec'.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cd29b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_eunji.py:\n",
      "out_dir = 'out-eunji'\n",
      "eval_interval = 10\n",
      "log_interval = 1\n",
      "\n",
      "always_save_checkpoint = False\n",
      "\n",
      "dataset = '은지말'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 8\n",
      "block_size = 64\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "max_iters = 20\n",
      "lr_decay_iters = 500\n",
      "dropout = 0.1\n",
      "\n",
      "learning_rate = 1e-3\n",
      "device = 'cpu'  # 은지는 지금 IPU 세션이니까 CPU로 지정해\n",
      "compile = False\n",
      "eval_iters = 20\n",
      "\n",
      "\n",
      "\n",
      "tokens per iteration will be: 512\n",
      "found vocab_size = 44 (inside data/은지말/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.79M\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "num decayed parameter tensors: 18, with 800,256 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "iter 0: loss 3.8638, time 56.36ms, mfu -100.00%\n",
      "iter 1: loss 3.8690, time 46.47ms, mfu -100.00%\n",
      "iter 2: loss 3.8488, time 43.89ms, mfu -100.00%\n",
      "iter 3: loss 3.8557, time 42.54ms, mfu -100.00%\n",
      "iter 4: loss 3.8438, time 42.16ms, mfu -100.00%\n",
      "iter 5: loss 3.8228, time 42.12ms, mfu 0.02%\n",
      "iter 6: loss 3.8428, time 43.71ms, mfu 0.02%\n",
      "iter 7: loss 3.8376, time 43.17ms, mfu 0.02%\n",
      "iter 8: loss 3.8292, time 43.97ms, mfu 0.02%\n",
      "iter 9: loss 3.8119, time 43.36ms, mfu 0.02%\n",
      "iter 10: loss 3.7833, time 43.35ms, mfu 0.02%\n",
      "iter 11: loss 3.8020, time 43.25ms, mfu 0.02%\n",
      "iter 12: loss 3.7757, time 43.18ms, mfu 0.02%\n",
      "iter 13: loss 3.7714, time 43.39ms, mfu 0.02%\n",
      "iter 14: loss 3.7614, time 42.60ms, mfu 0.02%\n",
      "iter 15: loss 3.7326, time 42.57ms, mfu 0.02%\n",
      "iter 16: loss 3.7171, time 43.26ms, mfu 0.02%\n",
      "iter 17: loss 3.7104, time 43.23ms, mfu 0.02%\n",
      "iter 18: loss 3.6822, time 42.57ms, mfu 0.02%\n",
      "iter 19: loss 3.6863, time 42.96ms, mfu 0.02%\n",
      "iter 20: loss 3.6522, time 43.12ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_eunji.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dff4261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512\r\n",
      "-rw-r--r-- 1 work work 398 Apr 15 23:02 은지.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e47afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1030, in load\r\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1248, in _legacy_load\r\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\r\n",
      "_pickle.UnpicklingError: invalid load key, '\\xec'.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "525dfefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512\r\n",
      "-rw-r--r-- 1 work work 398 Apr 15 23:02 은지.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a5ed07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_eunji.py:\n",
      "out_dir = 'out-eunji'\n",
      "eval_interval = 10\n",
      "log_interval = 1\n",
      "\n",
      "always_save_checkpoint = True\n",
      "\n",
      "dataset = '은지말'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 8\n",
      "block_size = 64\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "max_iters = 10\n",
      "lr_decay_iters = 500\n",
      "dropout = 0.1\n",
      "\n",
      "learning_rate = 1e-3\n",
      "device = 'cpu'  # 은지는 지금 IPU 세션이니까 CPU로 지정해\n",
      "compile = False\n",
      "eval_iters = 20\n",
      "\n",
      "\n",
      "\n",
      "tokens per iteration will be: 512\n",
      "found vocab_size = 44 (inside data/은지말/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.79M\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "num decayed parameter tensors: 18, with 800,256 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "iter 0: loss 3.8638, time 56.20ms, mfu -100.00%\n",
      "iter 1: loss 3.8690, time 46.36ms, mfu -100.00%\n",
      "iter 2: loss 3.8488, time 43.79ms, mfu -100.00%\n",
      "iter 3: loss 3.8557, time 43.35ms, mfu -100.00%\n",
      "iter 4: loss 3.8438, time 43.08ms, mfu -100.00%\n",
      "iter 5: loss 3.8228, time 42.90ms, mfu 0.02%\n",
      "iter 6: loss 3.8428, time 43.19ms, mfu 0.02%\n",
      "iter 7: loss 3.8376, time 43.01ms, mfu 0.02%\n",
      "iter 8: loss 3.8292, time 42.97ms, mfu 0.02%\n",
      "iter 9: loss 3.8119, time 43.02ms, mfu 0.02%\n",
      "iter 10: loss 3.7833, time 42.70ms, mfu 0.02%\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_eunji.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7a8cba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512\r\n",
      "-rw-r--r-- 1 work work 398 Apr 15 23:02 은지.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7d6781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1030, in load\r\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1248, in _legacy_load\r\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\r\n",
      "_pickle.UnpicklingError: invalid load key, '\\xec'.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ea4c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_eunji.py:\n",
      "out_dir = 'out-eunji'\n",
      "eval_interval = 10\n",
      "log_interval = 1\n",
      "\n",
      "always_save_checkpoint = True\n",
      "\n",
      "dataset = '은지말'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 8\n",
      "block_size = 64\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "max_iters = 10\n",
      "lr_decay_iters = 500\n",
      "dropout = 0.1\n",
      "\n",
      "learning_rate = 1e-3\n",
      "device = 'cpu'  # 은지는 지금 IPU 세션이니까 CPU로 지정해\n",
      "compile = False\n",
      "eval_iters = 20\n",
      "\n",
      "\n",
      "\n",
      "tokens per iteration will be: 512\n",
      "found vocab_size = 44 (inside data/은지말/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.79M\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "num decayed parameter tensors: 18, with 800,256 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "iter 0: loss 3.8638, time 55.58ms, mfu -100.00%\n",
      "iter 1: loss 3.8690, time 45.95ms, mfu -100.00%\n",
      "iter 2: loss 3.8488, time 43.68ms, mfu -100.00%\n",
      "iter 3: loss 3.8557, time 43.59ms, mfu -100.00%\n",
      "iter 4: loss 3.8438, time 42.76ms, mfu -100.00%\n",
      "iter 5: loss 3.8228, time 42.94ms, mfu 0.02%\n",
      "iter 6: loss 3.8428, time 42.96ms, mfu 0.02%\n",
      "iter 7: loss 3.8376, time 42.77ms, mfu 0.02%\n",
      "iter 8: loss 3.8292, time 42.78ms, mfu 0.02%\n",
      "iter 9: loss 3.8119, time 42.95ms, mfu 0.02%\n",
      "iter 10: loss 3.7833, time 43.01ms, mfu 0.02%\n",
      "✅ 강제 저장됨: ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_eunji.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8be74a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 512\r\n",
      "-rw-r--r-- 1 work work 398 Apr 15 23:02 은지.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "249bd31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1030, in load\r\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1248, in _legacy_load\r\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\r\n",
      "_pickle.UnpicklingError: invalid load key, '\\xec'.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7622a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 38, in <module>\r\n",
      "    checkpoint = torch.load(ckpt_path, map_location=device)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1030, in load\r\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1248, in _legacy_load\r\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\r\n",
      "_pickle.UnpicklingError: invalid load key, '\\xec'.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf616de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-eunji\r\n",
      "Overriding: start = 은지: \r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 39, in <module>\r\n",
      "    gptconf = GPTConfig(**checkpoint['model_args'])\r\n",
      "NameError: name 'checkpoint' is not defined. Did you mean: 'breakpoint'?\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ca6bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "953e42cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpt.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ ckpt.pt 저장 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), os.path.join(out_dir, 'ckpt.pt'))\n",
    "print(\"✅ ckpt.pt 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23e44a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_eunji.py:\n",
      "out_dir = 'out-eunji'\n",
      "eval_interval = 10\n",
      "log_interval = 1\n",
      "\n",
      "always_save_checkpoint = True\n",
      "\n",
      "dataset = '은지말'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 8\n",
      "block_size = 64\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "max_iters = 10\n",
      "lr_decay_iters = 500\n",
      "dropout = 0.1\n",
      "\n",
      "learning_rate = 1e-3\n",
      "device = 'cpu'  # 은지는 지금 IPU 세션이니까 CPU로 지정해\n",
      "compile = False\n",
      "eval_iters = 20\n",
      "\n",
      "\n",
      "\n",
      "tokens per iteration will be: 512\n",
      "found vocab_size = 44 (inside data/은지말/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.79M\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "num decayed parameter tensors: 18, with 800,256 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "iter 0: loss 3.8638, time 55.48ms, mfu -100.00%\n",
      "iter 1: loss 3.8690, time 46.08ms, mfu -100.00%\n",
      "iter 2: loss 3.8488, time 43.29ms, mfu -100.00%\n",
      "iter 3: loss 3.8557, time 42.94ms, mfu -100.00%\n",
      "iter 4: loss 3.8438, time 43.16ms, mfu -100.00%\n",
      "iter 5: loss 3.8228, time 42.83ms, mfu 0.02%\n",
      "iter 6: loss 3.8428, time 43.05ms, mfu 0.02%\n",
      "iter 7: loss 3.8376, time 43.43ms, mfu 0.02%\n",
      "iter 8: loss 3.8292, time 42.64ms, mfu 0.02%\n",
      "iter 9: loss 3.8119, time 42.62ms, mfu 0.02%\n",
      "iter 10: loss 3.7833, time 42.93ms, mfu 0.02%\n",
      "✅ 강제 저장됨: ckpt.pt\n",
      "✅ ckpt.pt 저장 완료!\n",
      "total 512\n",
      "-rw-r--r-- 1 work work 3.1M Apr 16 00:15 ckpt.pt\n",
      "-rw-r--r-- 1 work work  398 Apr 15 23:02 은지.txt\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_eunji.py\n",
    "!ls -lh out-eunji/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70762eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "number of parameters: 0.80M\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 42, in <module>\r\n",
      "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1016, in load\r\n",
      "    return _load(opened_zipfile,\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1424, in _load\r\n",
      "    result = unpickler.load()\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1394, in persistent_load\r\n",
      "    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1368, in load_tensor\r\n",
      "    wrap_storage=restore_location(storage, location),\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1298, in restore_location\r\n",
      "    return default_restore_location(storage, map_location)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 383, in default_restore_location\r\n",
      "    result = fn(storage, location)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 276, in _cuda_deserialize\r\n",
      "    device = validate_cuda_device(location)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 260, in validate_cuda_device\r\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\r\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c09d668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\r\n",
      "  warnings.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 42, in <module>\r\n",
      "    gptconf = GPTConfig(**checkpoint['model_args'])\r\n",
      "KeyError: 'model_args'\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-eunji --start=\"은지: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5533279",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ckpt_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[43mckpt_path\u001b[49m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m gptconf \u001b[38;5;241m=\u001b[39m GPTConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_args\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT(gptconf)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ckpt_path' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a61493da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_eunji.py:\n",
      "out_dir = 'out-eunji'\n",
      "eval_interval = 10\n",
      "log_interval = 1\n",
      "\n",
      "always_save_checkpoint = True\n",
      "\n",
      "dataset = '은지말'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 8\n",
      "block_size = 64\n",
      "n_layer = 4\n",
      "n_head = 4\n",
      "n_embd = 128\n",
      "max_iters = 10\n",
      "lr_decay_iters = 500\n",
      "dropout = 0.1\n",
      "\n",
      "learning_rate = 1e-3\n",
      "device = 'cpu'  # 은지는 지금 IPU 세션이니까 CPU로 지정해\n",
      "compile = False\n",
      "eval_iters = 20\n",
      "\n",
      "\n",
      "\n",
      "tokens per iteration will be: 512\n",
      "found vocab_size = 44 (inside data/은지말/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.79M\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "num decayed parameter tensors: 18, with 800,256 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "iter 0: loss 3.8638, time 54.74ms, mfu -100.00%\n",
      "iter 1: loss 3.8690, time 45.91ms, mfu -100.00%\n",
      "iter 2: loss 3.8488, time 43.34ms, mfu -100.00%\n",
      "iter 3: loss 3.8557, time 43.80ms, mfu -100.00%\n",
      "iter 4: loss 3.8438, time 42.74ms, mfu -100.00%\n",
      "iter 5: loss 3.8228, time 42.70ms, mfu 0.02%\n",
      "iter 6: loss 3.8428, time 42.82ms, mfu 0.02%\n",
      "iter 7: loss 3.8376, time 43.21ms, mfu 0.02%\n",
      "iter 8: loss 3.8292, time 43.48ms, mfu 0.02%\n",
      "iter 9: loss 3.8119, time 43.30ms, mfu 0.02%\n",
      "iter 10: loss 3.7833, time 43.07ms, mfu 0.02%\n",
      "✅ 전체 checkpoint 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_eunji.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8b1fe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 54\r\n",
      "    elif init_from.startswith('gpt2'):\r\n",
      "    ^^^^\r\n",
      "SyntaxError: invalid syntax\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "becf5b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 54\r\n",
      "    elif init_from.startswith('gpt2'):\r\n",
      "    ^^^^\r\n",
      "SyntaxError: invalid syntax\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "678f75c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/home/work/po/nanoGPT-master/sample.py\", line 54\r\n",
      "    elif init_from.startswith('gpt2'):\r\n",
      "    ^^^^\r\n",
      "SyntaxError: invalid syntax\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed539f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.09/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
